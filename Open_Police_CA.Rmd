---
title: "Are Police Biased Against Black Persons"
---


## Packages
```{r message=FALSE, warning=FALSE}
library(broom)
library(car) # box tidwell test
library(caret)
library(dbplyr)
library(data.table)
library(DataExplorer)
library(DBI)
library(DescTools) # for fit of logistic regression
library(devtools)
library(effects) # for log model plot
library(fastDummies)
library(hrbrthemes) # colour palets
library(inborutils) # for large files
library(kableExtra) # https://haozhu233.github.io/kableExtra/awesome_table_in_html.html
library(knitr)
library(performance)
library(data.table)
library(plotly)
library(pROC) #roc curves
library(readr)
library(rio)
library(RSQLite)
library(seriation) # test library that I never used. Infrastructure for ordering objects with ordination techniques. Provides optimally reordered heatmaps
library(tidyverse)
```
## Reading the Data
```{r}
# shouldn't need this after writing to SQLite db
# Ideas for larger DB in csv: bigmatrix package
# dat <- readRDS("C:/Users/Chris/OneDrive/R project/Open Policing/yg821jf8611_ca_statewide_2020_04_01.rds")
```
This is my first time encountering a large dataset (3 million rows). After much stumbling in the dark and many articles I've linked the file to a SQLite database, made an object that is a subset of the file (20,000 rows) for initial analysis, then saved it as an RDS object that isn't so big.

```{r}
# Ok SQLite does not have a storage class for dates or times, but it seems reasonably fast
library(dplyr)
#The code below should only need to be done once:


#file_name <- "California_Policing"
# sqldb <- dbConnect(SQLite(), dbname = file_name)
# Writing
# dbWriteTable(sqldb, name = "Calfornia_Statewide_Policing", dat, row.names = FALSE, overwrite = TRUE, append = FALSE, field.types = NULL)

# Reading, only need to do once. Taking the first 20,000 rows.
#df <- tbl(sqldb, "Calfornia_Statewide_Policing") %>% 
#  select(-date, -raw_row_number) %>% 
#  filter(row_number() %in% c(1:20000)) %>% 
#  collect()
# saving it as a RDS file for future use
# saveRDS(df,"C:/Users/Chris/OneDrive/R project/Open Policing/Open Policing/df.rds")
# Ok this is our regular working object for now: 20,000 rows out of 3 million to work with 
df <- readRDS("C:/Users/Chris/OneDrive/R project/Open Policing/Open Policing/df.rds")
# dbBegin(db) begins a transaction
# dbRollback(db) roll back reverts to original state
# dbCommit(db) 'commits' the data
```



```{r}
# Tidbit for future use
#The code below is an example for if you need to copy data over into a database:
#copy_to(con, nycflights13::flights, "flights",
#  temporary = FALSE, 
#  indexes = list(
#    c("year", "month", "day"), 
#    "carrier", 
#    "tailnum",
#    "dest"
#  )
#)
```


### Exporatory Analysis

DataExplorer package for intiial exploratory look.

Going to use glimpse on the data to get a look at the data. Glimpse reveals various location, vehicle, warning, and race information. Our variables are mostly categorical, with a lot of NA's that I might want to replace with 0's.
```{r}
glimpse(df)
```



```{r}
# This is unused but it's nice to know the way to find date range
#dat %>% 
#  select(date) %>% 
#  summarise(date_range = max(date) - min(date))
```


```{r}
plot_str(df)
```
I'm not sure how nice this graph looks to be honest - will probably delete.

```{r}
introduce(df)
```
Introduce is very useful, it's telling me that there are a lot of missing values. The memory usage is in byes which is 2.5MB. Perhaps I could select more than 20,000 rows.

```{r}
plot_missing(df)
```
Whilst the NA's in district, county_name and search_person seem to be genuine missing data, the other variables seem to be using NA as a geuine outcome. Let's take a look at these columns:
```{r}
df %>% select(outcome, warning_issued, citation_issued, arrest_made, search_basis, contraband_found, frisk_performed) %>% 
  distinct()
```
The frisk_performed column has only NA's and 1's. We can treat the NA's as 0 i.e no frisk performed. arrest_made, citation_issued, warning_issued, outcome tend to have NA's together for a row. I believe it is a reasonable assumption that nothing occured during these pull overs. Therefore we can replace these NA's with 0's. The search_basis is giving us 'other' or NA, so we should probably remove this column. Search_person can also replace NA's with 0's.


```{r}
# Modifying all these NA entries:
df <- df %>% 
  replace_na(list(outcome = "nothing", warning_issued = 0, arrest_made = 0, citation_issued = 0, warning_issued = 0, contraband_found = 0, frisk_performed = 0, search_conducted = 0 )) %>% 
  select(-search_basis)
df <- df %>% replace_na(list(search_person = 0))
```

```{r}
plot_missing(df)
```
The district and county_name entries with NA entries can be treated as unusable data, we can remove them.
```{r}
df <- df %>% 
  na.omit()
```
```{r}
plot_missing(df)
```
And we're done cleaning the NA!



### Categorical Data Analysis

Needed some population data to compare our data to. These demographic statistics are from wikipedia.
```{r}
df %>% distinct(subject_race)
# In the order of: asian/islander, black, hispanic, other, white
demo <- c(0.1452+0.0036, 0.0551, 0.3929, 0.0368, 0.3664)
# Numbers are taken from https://en.wikipedia.org/wiki/Demographics_of_California#/media/File:Ethic_California_Organized_Pie.png. The other category I obtained from 1 - sum(demo).
```
#### Visualising the covariation between two categorical variables

```{r}
ggplot(data = df) +
  geom_count(mapping = aes(x = subject_race, y = outcome, color = ..n.., size = ..n..)) +
  scale_size_area() +
  scale_size_continuous(range = c(1,10)) +
  ggtitle("Covariation Between Outcome and Race") +
  labs(x ="Race of Subject", y = "Outcome") +
  guides(color = "legend")
```
While this is not the most informative graph, it is interesting to note that quite few direct arrests. Most of the outcomes are summons or nothing. As one can expect, the circles are largest for the hispanic and white groups - the two groups with the largest samples. Let's do a proportion graph:

```{r}
ggplot(data = df) +
  geom_count(mapping = aes(x = subject_race, y = outcome, color = ..prop.., size = ..prop.., group = 1)) +
  scale_size_area() +
  scale_size_continuous(range = c(1,10)) +
  ggtitle("Covariation Between Outcome and Race") +
  labs(x ="Race of Subject", y = "Outcome") +
  guides(color = "legend")
```
Show's the same stuff, but it's nice to know it's easy to go between the two. We may also be interested in a heatmap version:

```{r}
df %>% 
  count(subject_race, outcome) %>% 
ggplot(aes(x = subject_race, y = outcome)) +
  geom_tile(aes(fill = n))
```
```{r}

nrow(unique(df %>% count(subject_race,outcome)))
# 25
colours <- colorRampPalette(c("blue", "green", "red"))(25)
df %>% 
  count(subject_race, outcome) %>% 
ggplot(aes(x = subject_race, y = outcome)) +
  geom_tile(aes(fill = n)) +
  scale_fill_distiller(palette = "RdPu") 
#  theme_ipsum() moves axis labels to the side
#  Other options
#  scale_fill_gradient(low = "White", high = "blue")
#
 
#  scale_fill_brewer(palette = "PRGn") # scale_fill_brewer requires factor for fill. Ok it's limited to 11 different facotrs this is better for something that is discrete
```
Same information is displayed but it's definitely a more visually engaging method. The larger numbers of summons +hispanic/white really pop out.
#### Test of single proportion
```{r}
df %>% 
  group_by(subject_race) %>% 
  summarise(n = n()) %>% 
  mutate(rsum = sum(n))

```
We can see that 1631 out of 19938 individuals pulled over were black. Wikipedia states that 5.51% of the population in CA is of black race. We can test the hypothesis that
*$H_0:$ The proportion of tested black race being 0.0551 is true
*$H_1:$ The proportion of tested black race being 0.0551 is not true
```{r}
prop.test(1631, 19938, 0.0551, conf.level = 0.95)
```
The extremely low p-vale suggests we reject the null hypothesis. The estimated proportion is 0.081 with a 95% confidence interval (0.078, 0.085). This suggests that there is some bias towards selecting black drivers to be pulled over.


```{r}
# library(broom)
df_chisq <- df %>% 
  group_by(subject_race,outcome) %>%  # the variables you want on the conteingency table
  summarise(n = n()) %>% # need the totals
  mutate(proportion = n/sum(n)) %>%
  select(-proportion) %>% # Oh you definitely need to get rid of proportion here so it spreads properly
  spread(outcome, n) %>%  # contingency table obtained! Also got proportions...and then got rid of them should make them separate
  ungroup() %>% # select will not remove in a grouped tibble
  select(-1) %>% 
  chisq.test # %>% 
  glance() # can't decide if I want it glanced...not this time
df_chisq
```
Testing of association between subject_race and outcome. The p-value is less than 0.05 so we reject the null hypothesis of no association and conclude that there is a association between the row variables (race) and column variables (outcome).
Let's have a look at the expected counts:
```{r}
t1 <- round(as_tibble(df_chisq$expected),0)
t1
```
Compare it to the actual counts in data:
```{r}
t2 <- df %>% 
  group_by(subject_race,outcome) %>%  # the variables you want on the conteingency table
  summarise(n = n()) %>% # need the totals
  mutate(proportion = n/sum(n)) %>%
  select(-proportion) %>% # Oh you definitely need to get rid of proportion here so it spreads properly
  spread(outcome, n) %>%  # contingency table obtained! Also got proportions...and then got rid of them should make them separate
  ungroup() %>% 
  select(-1)
```
Let's compare them side by side:
```{r}
t0 <- df %>% 
  group_by(subject_race,outcome) %>%  # the variables you want on the conteingency table
  summarise(n = n()) %>% # need the totals
  mutate(proportion = n/sum(n)) %>%
  select(-proportion) %>% # Oh you definitely need to get rid of proportion here so it spreads properly
  spread(outcome, n) %>%  # contingency table obtained! Also got proportions...and then got rid of them should make them separate
  ungroup() %>% 
  select(1)
t1a <- tibble(t0,t1)
t2a <- tibble(t0, t2)
kables(list(
            kable(caption = "Expected counts", t1a) %>% 
              kable_classic() %>% 
                column_spec(3, color = spec_color(t1$arrest)),
            kable(caption = "Actual counts", t2a) %>% 
              kable_classic() %>% 
                column_spec(3, color = spec_color(t2$arrest)))
       ) %>% kable_classic()
# wow this is cool. Hope it knits well
# kable classic makes it nice, but kable_styling() makes it unreadable for my dark-mode/markdown setup
# Other options: kable_paper, kable_classic_2, kable_material, kable_material_dark
# this is not bad 
```
At a glance we can see that some of the biggest differences occur for hispanic & warning, and black & citation. The 'other' group is defined a bit differently between the dataset and wikipedia so we shouldn't draw too much from it.
```{r}
# Cool kable concept that I want to explore later - but not now
#t1
#t1dt <- lapply(t1[1:ncol(t1),2:nrow(t1)], function(x) {
#  cell_spec(x, bold = T,
#            color = spec_color(x, end = 0.9), #generates viridus color
#            font_size = spec_font_size(x, begin = 10, end = 16))
#})
#kbl(t1dt, escape = F, align = "c") %>% 
#  kable_classic("striped", full_width = F)
```


And the most contributing cells to the total chi-square score:
```{r}
subject_race <- c("asian/pacific islander", "black", "hispanic", "other", "white")
chisqres <- as_tibble(df_chisq$residuals) %>% add_column(subject_race, .before = 1)
chisqres
```
The cells with the highest absolute standardized residuals contribute the most to the total chi-square score. Let's visualise this:
```{r}
ggplot(data = melt(chisqres), aes(x = subject_race,y = variable )) +
  geom_raster(aes(fill = value)) +
  scale_fill_gradient(low = "green", high = "red")
```
It can be seen that the column black is strongly associated with summons/arrest/warning but not strongly associated with nothing and citation.

### Classification models

## Multicollinearity?

```{r}
model_df <- df %>% 
  filter(raw_search_basis == "Probable Cause (positive)" | raw_search_basis == "Probable Cause (negative)") %>% 
  mutate(across(where(is_character), as_factor)) %>% 
  select(subject_race, subject_sex, outcome, raw_search_basis, search_conducted) 
set.seed(42) 
rows <- sample(nrow(model_df)) # 11970 rows
# We'll use the first 8000 randomised entries for the model
train <- model_df[rows[1:8000], ]
test <- model_df[-rows[1:8000], ]
# And the rest for testing
model <- glm(raw_search_basis ~ subject_race, family = binomial(link = "logit"), data = train)
summary(model)
```
We now have a model depicting the relationship between a search being positive or negative and the race of the person being pulled over. The p-values are interpreted as whether there is a difference between the log-odds of the outcome between the intercept and the explanatory variable. All of the predictors are significant using p < 0.1 as the criteria but if we use p < 0.05 which is more standard then the model would reject subject_raceblack as being useful in the model.

```{r}
calc_class_err = function(actual, predicted){
  mean(actual != predicted)
}
probabilities <-  predict(model, newdata = test, type = "response")
predicted.class <- ifelse(probabilities > 0.5, "Probable Cause (negative)", "Probable Cause (positive)")
test_table <-  table(predicted.class, test$raw_search_basis)
test_table #<- rbind(test_table, c(0,0))
# for prediction this model needs some adjustments
#rownames(test_table) <-  c("Probable Cause (positive)",("Probable Cause (negative)"))
test_table
#alc_class_err(actual = test$raw_search_basis, predicted = predicted.class)
# this doesn't tell the whole story.
```
There is cause for concern in the table as the predicted class is only showing "Probable Cause (positive)" predictions. Lets obtain more metrics.


```{r}
test1 <- test %>% mutate(raw_search_basis = ifelse(raw_search_basis == "Probable Cause (negative)", 1, 0))
cutoffs <- seq(0, 1, by = 0.05)
eff <- sapply(cutoffs, function(cutoff){
  sum((probabilities > cutoff) == test1$raw_search_basis)/length(probabilities)
})
eff
```
```{r}
# Produce a ROC curve based on the current model.
test_roc = roc(test$raw_search_basis ~ probabilities, plot = TRUE, print.auc = TRUE)
# 0.554 isn't very good. But we should use it to choose optimal threshold
```

```{r}
# extract the co-ordinates
mycoords <- coords(test_roc, "all")
best.coords <- coords(test_roc, "best", best.method = "youden")
best.coords
```
Sensitivity is defined as the True Positive rate (sensitivity = $\frac{TP}{TP + FN}$). This is decently high for this model. Specificity is defined as the True Negate rate (specificity = $\frac{TN}{TN + FP}$}). We use 1 - Specificity to define the False Positive Rate.  Lowering the classification threshold classifies more items as 'positive', increasing more false positives and true positives. Our optimal cutoff here using the "youden" method is 0.339, this will improve the classification power of the model.

```{r}
probabilities <-  predict(model, newdata = test, type = "response")
predicted.class <- ifelse(probabilities > 0.339, "Probable Cause (negative)", "Probable Cause (positive)")
test_table <-  table(predicted.class, test$raw_search_basis)
test_table
```
Classification rate is $(1735+589)/(1735+589+932+714) = 0.585$, which is a lot better than 0.3 from earlier. The true positive rate is $1735/1735+932 = 0.65$ which is the same as the sensitivity rate stated. The true negative rate is $589/(589 + 714 = 0.45$ which is the specificity rate.

## Model fit

(Move ROC stuff here).

There are two very different approaches to answer this question. One is to get statistics on how well you can predict the dependent variable based on the independent variables (measures of predictive power). Examples would be R-square, the area under the ROC curve, and several rank-order correlations. Higher is better but there is rarely a fixed cut-off that distinguishes an acceptable model from one that is not acceptable.

The other approach is to compute a goodness-of-fit statistic. These test whether you can do better by making the model more complicated, specifically, adding non-linearities, adding interactions or chainging the link function. There are the deviance, Pearson chi-square or Hosmer-Lemeshow test. These are tests of the null hypothesis that the fitted model is correct, and their output is a p-value, with a higher value indicating a better fit. A p-value below some specified $\alpha$ level would suggest that the model is not acceptable.

Important to note is that measures of predictive power and goodness-of-fit statistics are testing different things. As such a model with very high R-squared for example might have terrible goodness-of-fit statistics. 

A bit more on goodness-of-fit (GOF) tests: they test whether there are any non-linearities or interactions. One can always produce a good fit by adding enough interactions and nonlinearities. The question is if you really need them to properly represent the data. GOF tests are designed to answer that question.

Small summary of GOF tests:

-  Deviance: different of likelihoods between fitted and saturated model (saturated model likelihood is 1). Is always $\geq 0$, and is $0$ if the fit is perfect.
- & Pearson chi-square 
- Hosmer and Lemeshow: groups cases together according to predicted values from the logistic regression model. Predicted values are sorted from highest to lowest, and then separated into several groups of approximately equal size.  The number of observed events and non-events are calculated, as well as the expected number of events and non-events. This would be the sum of the predicted probabilities and the expected number of non-events would be the group size minus the expected number of events. Pearson's chi-square is applied to compare observed counts with expected counts. Degrees of freedom is the number of groups minus 2. Has drawbacks - changing the number of groups can heavily influence the outcome, and adding statistically significant interaction or non-linearity to a model can sometimes increase the HL and reduce the p-value. This suggest we're better off without this statistically significant interaction term. The reverse can also happen (adding a non-significant interaction will improve the HL fit).

Lets look at our measures of predictiveness:


```{r}
PseudoR2(model, which = c("McFadden", "CoxSnell","Tjur"))
```
All of these suggest that the predictive power is poor - which is definitely something we can agree on. The purpose of the model is to see the interaction rather than the predictive power anyway.

Now let's take a look at the goodness of fit measures.
```{r}
# To see if the deviance matters we take the full model and compare to the current one:
model1 <- glm(raw_search_basis ~., family = binomial,data = train)
summary(model1)
```
```{r}
anova(model, model1)
```
```{r}
model2 <- glm(raw_search_basis ~ subject_race + outcome, family = "binomial", data = train)
anova(model, model1, model2)
```
For a better model that fits the data, we would add "outcome" to the variables. The deviance is also almost as good as the saturated model. However this is not the purpose of the exercise so I will just leave it as an aside.

Let's plot our model:
```{r}
plot(allEffects(model)) # from effects package. typical = median is an alternative option
```
On the left hand side we have predicted probabilities for being a probable cause (negative). It also includes 95% confidence interval bars. There is a clear effect of black and hispanics being those more likely to be false alarms.


## Assumption check

- outcome is binary/dichotomous
- linear relationship between logic of the outcome and each predictor variable
- no influencial values
- no multicollinearity
```{r}
check_collinearity(model) # jokes only 1 variable
out_list <- check_outliers(train) # 218 outliers
dfbetas_model <- as_tibble(dfbetas(model)[out_list == TRUE, ])
dfbetas_model %>% 
  ggplot(aes(y = .[[1]], x = seq_along(dfbetas_model$`(Intercept)`))) +
  geom_point() 
dfbetas_model %>% 
  ggplot(aes(y = .[[2]], x = seq_along(subject_raceblack))) +
  geom_point() 
dfbetas_model %>% 
  ggplot(aes(y = .[[3]], x = seq_along(subject_raceblack))) +
  geom_point() 
```
The only thing we can comment on here is that we can't remove these outliers - they are an essential part of the data.

```{r}
plot(model$fitted.values)
```

## Interpretation and Predictions

Let's quickly see how R is dummy coding the variables:
```{r}
contrasts(model_df$raw_search_basis)
```
Probable cause (positive) is coded as 0 while probable cause (negative) is coded as 1. We take exponential of the estimates for easier interpretation. Some of the notable interptations are:

```{r}
model
```

* An intercept of  -0.51  implies there is a $exp(-0.51) = 0.60$ probability that the pull-over for a hispanic is a false alarm
* subject_raceblack estimate of 0.13792 means being black increases the log odds by 0.13792.   being black has $exp(0.13792) = 1.148$ odds of being a negative false alarm than hispanics (the reference or intercept category)
* All other groups have negative estimates and so they are more likely to be Probable Cause(positive) than hispanics
  - White has an estimate of  -0.33708, and so $exp(-0.33708) = 0.714$ implies the average white has a $0.6*0.714= 0.42$ chance of being a Probable Cause (negative). Which means pullovers have more than a 50% chance of being correct
  - Asian/Pacific Islander has an estimate of -0.49, and so $exp(-0.49) = 0.61$ implies the average asian/pacific islander has a $0.6*0.61 = 0.366$ chance of being a Probable Cause (negative). Which means pullovers have a 36% chance of being a Probable Cause (negative). I.e most of the pullovers due to suspicion were justified

```{r}
# Showing this in quick view
exp(coef(model))
```

```{r}
ggplot(model_df, aes(raw_search_basis, fill = subject_race)) +
  geom_bar(position = "fill")
```
This makes sense when we look at the plot above, proportionally most of the false alarms 

Now lets see the classification rate when we test the predictive ability of the model.
```{r}
probabilities <-  predict(model, newdata = test, type = "response")
predicted.class <- ifelse(probabilities > 0.327, "Probable Cause (negative)", "Probable Cause (positive)")
table(predicted.class, test$raw_search_basis)
```
There is a problem with this model. If we change 0.32 to be the threshhold (based on ROC optimisation), then we get a 59% classification success rate. We shouldn't use this model for predicting anyway.

### Conclusion
There is bias against black persons. In terms of model there isn't anything nice from logistic regression for prediction. Perhaps I should try classification trees, neural networks or support vector machines next.

