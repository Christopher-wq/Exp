<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Are Police Biased Against Black Persons</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Projects</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Customer-Personality-Analysis-read.html">Customer Personality Data</a>
</li>
<li>
  <a href="Open_Police_CA.html">Open Policing Analysis</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Are Police Biased Against Black
Persons</h1>

</div>


<div id="packages" class="section level2">
<h2>Packages</h2>
<pre class="r"><code>library(broom)
library(car) # box tidwell test
library(caret)
library(dbplyr)
library(data.table)
library(DataExplorer)
library(DBI)
library(DescTools) # for fit of logistic regression
library(devtools)
library(effects) # for log model plot
library(fastDummies)
library(hrbrthemes) # colour palets
library(inborutils) # for large files
library(kableExtra) # https://haozhu233.github.io/kableExtra/awesome_table_in_html.html
library(knitr)
library(performance)
library(data.table)
library(plotly)
library(pROC) #roc curves
library(readr)
library(rio)
library(RSQLite)
library(seriation) # test library that I never used. Infrastructure for ordering objects with ordination techniques. Provides optimally reordered heatmaps
library(tidyverse)</code></pre>
</div>
<div id="reading-the-data" class="section level2">
<h2>Reading the Data</h2>
<pre class="r"><code># shouldn&#39;t need this after writing to SQLite db
# Ideas for larger DB in csv: bigmatrix package
# dat &lt;- readRDS(&quot;C:/Users/Chris/OneDrive/R project/Open Policing/yg821jf8611_ca_statewide_2020_04_01.rds&quot;)</code></pre>
<p>This is my first time encountering a large dataset (3 million rows).
After much stumbling in the dark and many articles I’ve linked the file
to a SQLite database, made an object that is a subset of the file
(20,000 rows) for initial analysis, then saved it as an RDS object that
isn’t so big.</p>
<pre class="r"><code># Ok SQLite does not have a storage class for dates or times, but it seems reasonably fast
library(dplyr)
#The code below should only need to be done once:


#file_name &lt;- &quot;California_Policing&quot;
# sqldb &lt;- dbConnect(SQLite(), dbname = file_name)
# Writing
# dbWriteTable(sqldb, name = &quot;Calfornia_Statewide_Policing&quot;, dat, row.names = FALSE, overwrite = TRUE, append = FALSE, field.types = NULL)

# Reading, only need to do once. Taking the first 20,000 rows.
#df &lt;- tbl(sqldb, &quot;Calfornia_Statewide_Policing&quot;) %&gt;% 
#  select(-date, -raw_row_number) %&gt;% 
#  filter(row_number() %in% c(1:20000)) %&gt;% 
#  collect()
# saving it as a RDS file for future use
# saveRDS(df,&quot;C:/Users/Chris/OneDrive/R project/Open Policing/Open Policing/df.rds&quot;)
# Ok this is our regular working object for now: 20,000 rows out of 3 million to work with 
df &lt;- readRDS(&quot;C:/Users/Chris/OneDrive/R project/Open Policing/Open Policing/df.rds&quot;)
# dbBegin(db) begins a transaction
# dbRollback(db) roll back reverts to original state
# dbCommit(db) &#39;commits&#39; the data</code></pre>
<pre class="r"><code># Tidbit for future use
#The code below is an example for if you need to copy data over into a database:
#copy_to(con, nycflights13::flights, &quot;flights&quot;,
#  temporary = FALSE, 
#  indexes = list(
#    c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), 
#    &quot;carrier&quot;, 
#    &quot;tailnum&quot;,
#    &quot;dest&quot;
#  )
#)</code></pre>
<div id="exporatory-analysis" class="section level3">
<h3>Exporatory Analysis</h3>
<p>DataExplorer package for intiial exploratory look.</p>
<p>Going to use glimpse on the data to get a look at the data. Glimpse
reveals various location, vehicle, warning, and race information. Our
variables are mostly categorical, with a lot of NA’s that I might want
to replace with 0’s.</p>
<pre class="r"><code>glimpse(df)</code></pre>
<pre><code>## Rows: 20,000
## Columns: 19
## $ county_name      &lt;chr&gt; &quot;Stanislaus County&quot;, &quot;Stanislaus County&quot;, &quot;Stanislaus County&quot;, &quot;Stanislaus ~
## $ district         &lt;chr&gt; &quot;Modesto&quot;, &quot;Modesto&quot;, &quot;Modesto&quot;, &quot;Modesto&quot;, &quot;Modesto&quot;, &quot;Modesto&quot;, &quot;Modesto&quot;~
## $ subject_race     &lt;chr&gt; &quot;other&quot;, &quot;hispanic&quot;, &quot;hispanic&quot;, &quot;white&quot;, &quot;hispanic&quot;, &quot;hispanic&quot;, &quot;hispanic~
## $ subject_sex      &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;~
## $ department_name  &lt;chr&gt; &quot;California Highway Patrol&quot;, &quot;California Highway Patrol&quot;, &quot;California Highw~
## $ type             &lt;chr&gt; &quot;vehicular&quot;, &quot;vehicular&quot;, &quot;vehicular&quot;, &quot;vehicular&quot;, &quot;vehicular&quot;, &quot;vehicular~
## $ violation        &lt;chr&gt; &quot;Motorist / Public Service&quot;, &quot;Moving Violation (VC)&quot;, &quot;Moving Violation (VC~
## $ arrest_made      &lt;int&gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, NA, NA, NA, 0, 0, NA, ~
## $ citation_issued  &lt;int&gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, NA, NA, NA, 1, 0, NA, ~
## $ warning_issued   &lt;int&gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, NA, NA, NA, NA, 0, 0, NA, ~
## $ outcome          &lt;chr&gt; NA, &quot;summons&quot;, &quot;summons&quot;, &quot;summons&quot;, &quot;summons&quot;, &quot;summons&quot;, &quot;summons&quot;, &quot;summ~
## $ contraband_found &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~
## $ frisk_performed  &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~
## $ search_conducted &lt;int&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~
## $ search_person    &lt;int&gt; 0, 0, NA, 0, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ search_basis     &lt;chr&gt; NA, NA, &quot;other&quot;, NA, &quot;other&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
## $ reason_for_stop  &lt;chr&gt; &quot;Motorist / Public Service&quot;, &quot;Moving Violation (VC)&quot;, &quot;Moving Violation (VC~
## $ raw_race         &lt;chr&gt; &quot;Other&quot;, &quot;Hispanic&quot;, &quot;Hispanic&quot;, &quot;White&quot;, &quot;Hispanic&quot;, &quot;Hispanic&quot;, &quot;Hispanic~
## $ raw_search_basis &lt;chr&gt; &quot;Vehicle Inventory&quot;, &quot;Probable Cause (positive)&quot;, &quot;Probable Cause (positive~</code></pre>
<pre class="r"><code># This is unused but it&#39;s nice to know the way to find date range
#dat %&gt;% 
#  select(date) %&gt;% 
#  summarise(date_range = max(date) - min(date))</code></pre>
<pre class="r"><code>plot_str(df)</code></pre>
<p>I’m not sure how nice this graph looks to be honest - will probably
delete.</p>
<pre class="r"><code>introduce(df)</code></pre>
<pre><code>## # A tibble: 1 x 9
##    rows columns discrete_columns continuous_columns all_missing_columns total_missing_v~ complete_rows
##   &lt;int&gt;   &lt;int&gt;            &lt;int&gt;              &lt;int&gt;               &lt;int&gt;            &lt;int&gt;         &lt;int&gt;
## 1 20000      19               12                  7                   0            85795             0
## # ... with 2 more variables: total_observations &lt;int&gt;, memory_usage &lt;dbl&gt;</code></pre>
<p>Introduce is very useful, it’s telling me that there are a lot of
missing values. The memory usage is in byes which is 2.5MB. Perhaps I
could select more than 20,000 rows.</p>
<pre class="r"><code>plot_missing(df)</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-9-1.png" width="672" />
Whilst the NA’s in district, county_name and search_person seem to be
genuine missing data, the other variables seem to be using NA as a
geuine outcome. Let’s take a look at these columns:</p>
<pre class="r"><code>df %&gt;% select(outcome, warning_issued, citation_issued, arrest_made, search_basis, contraband_found, frisk_performed) %&gt;% 
  distinct()</code></pre>
<pre><code>## # A tibble: 19 x 7
##    outcome  warning_issued citation_issued arrest_made search_basis   contraband_found frisk_performed
##    &lt;chr&gt;             &lt;int&gt;           &lt;int&gt;       &lt;int&gt; &lt;chr&gt;                     &lt;int&gt;           &lt;int&gt;
##  1 &lt;NA&gt;                 NA              NA          NA &lt;NA&gt;                         NA              NA
##  2 summons               0               0           0 &lt;NA&gt;                         NA              NA
##  3 summons               0               0           0 other                        NA              NA
##  4 warning               1               0           0 &lt;NA&gt;                         NA              NA
##  5 citation              0               1           0 &lt;NA&gt;                         NA              NA
##  6 arrest                0               0           1 other                        NA              NA
##  7 &lt;NA&gt;                 NA              NA          NA other                        NA              NA
##  8 arrest                0               0           1 &lt;NA&gt;                         NA              NA
##  9 &lt;NA&gt;                 NA              NA          NA other                        NA               1
## 10 summons               0               0           0 other                        NA               1
## 11 summons               0               0           0 probable cause                1              NA
## 12 citation              0               1           0 other                        NA              NA
## 13 warning               1               0           0 other                        NA              NA
## 14 summons               0               0           0 probable cause                0              NA
## 15 arrest                0               0           1 consent                       1              NA
## 16 arrest                0               0           1 probable cause                1              NA
## 17 warning               1               0           0 probable cause                1              NA
## 18 &lt;NA&gt;                 NA              NA          NA probable cause                0              NA
## 19 arrest                0               0           1 probable cause                0              NA</code></pre>
<p>The frisk_performed column has only NA’s and 1’s. We can treat the
NA’s as 0 i.e no frisk performed. arrest_made, citation_issued,
warning_issued, outcome tend to have NA’s together for a row. I believe
it is a reasonable assumption that nothing occured during these pull
overs. Therefore we can replace these NA’s with 0’s. The search_basis is
giving us ‘other’ or NA, so we should probably remove this column.
Search_person can also replace NA’s with 0’s.</p>
<pre class="r"><code># Modifying all these NA entries:
df &lt;- df %&gt;% 
  replace_na(list(outcome = &quot;nothing&quot;, warning_issued = 0, arrest_made = 0, citation_issued = 0, warning_issued = 0, contraband_found = 0, frisk_performed = 0, search_conducted = 0 )) %&gt;% 
  select(-search_basis)
df &lt;- df %&gt;% replace_na(list(search_person = 0))</code></pre>
<pre class="r"><code>plot_missing(df)</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-12-1.png" width="672" />
The district and county_name entries with NA entries can be treated as
unusable data, we can remove them.</p>
<pre class="r"><code>df &lt;- df %&gt;% 
  na.omit()</code></pre>
<pre class="r"><code>plot_missing(df)</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-14-1.png" width="672" />
And we’re done cleaning the NA!</p>
</div>
<div id="categorical-data-analysis" class="section level3">
<h3>Categorical Data Analysis</h3>
<p>Needed some population data to compare our data to. These demographic
statistics are from wikipedia.</p>
<pre class="r"><code>df %&gt;% distinct(subject_race)</code></pre>
<pre><code>## # A tibble: 5 x 1
##   subject_race          
##   &lt;chr&gt;                 
## 1 other                 
## 2 hispanic              
## 3 white                 
## 4 black                 
## 5 asian/pacific islander</code></pre>
<pre class="r"><code># In the order of: asian/islander, black, hispanic, other, white
demo &lt;- c(0.1452+0.0036, 0.0551, 0.3929, 0.0368, 0.3664)
# Numbers are taken from https://en.wikipedia.org/wiki/Demographics_of_California#/media/File:Ethic_California_Organized_Pie.png. The other category I obtained from 1 - sum(demo).</code></pre>
<div id="visualising-the-covariation-between-two-categorical-variables"
class="section level4">
<h4>Visualising the covariation between two categorical variables</h4>
<pre class="r"><code>ggplot(data = df) +
  geom_count(mapping = aes(x = subject_race, y = outcome, color = ..n.., size = ..n..)) +
  scale_size_area() +
  scale_size_continuous(range = c(1,10)) +
  ggtitle(&quot;Covariation Between Outcome and Race&quot;) +
  labs(x =&quot;Race of Subject&quot;, y = &quot;Outcome&quot;) +
  guides(color = &quot;legend&quot;)</code></pre>
<pre><code>## Scale for &#39;size&#39; is already present. Adding another scale for &#39;size&#39;, which will replace the
## existing scale.</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-16-1.png" width="672" />
While this is not the most informative graph, it is interesting to note
that quite few direct arrests. Most of the outcomes are summons or
nothing. As one can expect, the circles are largest for the hispanic and
white groups - the two groups with the largest samples. Let’s do a
proportion graph:</p>
<pre class="r"><code>ggplot(data = df) +
  geom_count(mapping = aes(x = subject_race, y = outcome, color = ..prop.., size = ..prop.., group = 1)) +
  scale_size_area() +
  scale_size_continuous(range = c(1,10)) +
  ggtitle(&quot;Covariation Between Outcome and Race&quot;) +
  labs(x =&quot;Race of Subject&quot;, y = &quot;Outcome&quot;) +
  guides(color = &quot;legend&quot;)</code></pre>
<pre><code>## Scale for &#39;size&#39; is already present. Adding another scale for &#39;size&#39;, which will replace the
## existing scale.</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-17-1.png" width="672" />
Show’s the same stuff, but it’s nice to know it’s easy to go between the
two. We may also be interested in a heatmap version:</p>
<pre class="r"><code>df %&gt;% 
  count(subject_race, outcome) %&gt;% 
ggplot(aes(x = subject_race, y = outcome)) +
  geom_tile(aes(fill = n))</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>nrow(unique(df %&gt;% count(subject_race,outcome)))</code></pre>
<pre><code>## [1] 25</code></pre>
<pre class="r"><code># 25
colours &lt;- colorRampPalette(c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;))(25)
df %&gt;% 
  count(subject_race, outcome) %&gt;% 
ggplot(aes(x = subject_race, y = outcome)) +
  geom_tile(aes(fill = n)) +
  scale_fill_distiller(palette = &quot;RdPu&quot;) </code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code>#  theme_ipsum() moves axis labels to the side
#  Other options
#  scale_fill_gradient(low = &quot;White&quot;, high = &quot;blue&quot;)
#
 
#  scale_fill_brewer(palette = &quot;PRGn&quot;) # scale_fill_brewer requires factor for fill. Ok it&#39;s limited to 11 different facotrs this is better for something that is discrete</code></pre>
<p>Same information is displayed but it’s definitely a more visually
engaging method. The larger numbers of summons +hispanic/white really
pop out. #### Test of single proportion</p>
<pre class="r"><code>df %&gt;% 
  group_by(subject_race) %&gt;% 
  summarise(n = n()) %&gt;% 
  mutate(rsum = sum(n))</code></pre>
<pre><code>## # A tibble: 5 x 3
##   subject_race               n  rsum
##   &lt;chr&gt;                  &lt;int&gt; &lt;int&gt;
## 1 asian/pacific islander  1247 19938
## 2 black                   1631 19938
## 3 hispanic                6377 19938
## 4 other                   1279 19938
## 5 white                   9404 19938</code></pre>
<p>We can see that 1631 out of 19938 individuals pulled over were black.
Wikipedia states that 5.51% of the population in CA is of black race. We
can test the hypothesis that <em><span
class="math inline">\(H_0:\)</span> The proportion of tested black race
being 0.0551 is true </em><span class="math inline">\(H_1:\)</span> The
proportion of tested black race being 0.0551 is not true</p>
<pre class="r"><code>prop.test(1631, 19938, 0.0551, conf.level = 0.95)</code></pre>
<pre><code>## 
##  1-sample proportions test with continuity correction
## 
## data:  1631 out of 19938, null probability 0.0551
## X-squared = 272.56, df = 1, p-value &lt; 2.2e-16
## alternative hypothesis: true p is not equal to 0.0551
## 95 percent confidence interval:
##  0.07805494 0.08571442
## sample estimates:
##          p 
## 0.08180359</code></pre>
<p>The extremely low p-vale suggests we reject the null hypothesis. The
estimated proportion is 0.081 with a 95% confidence interval (0.078,
0.085). This suggests that there is some bias towards selecting black
drivers to be pulled over.</p>
<pre class="r"><code># library(broom)
df_chisq &lt;- df %&gt;% 
  group_by(subject_race,outcome) %&gt;%  # the variables you want on the conteingency table
  summarise(n = n()) %&gt;% # need the totals
  mutate(proportion = n/sum(n)) %&gt;%
  select(-proportion) %&gt;% # Oh you definitely need to get rid of proportion here so it spreads properly
  spread(outcome, n) %&gt;%  # contingency table obtained! Also got proportions...and then got rid of them should make them separate
  ungroup() %&gt;% # select will not remove in a grouped tibble
  select(-1) %&gt;% 
  chisq.test # %&gt;% </code></pre>
<pre><code>## `summarise()` has grouped output by &#39;subject_race&#39;. You can override using the `.groups` argument.</code></pre>
<pre class="r"><code>  glance() # can&#39;t decide if I want it glanced...not this time</code></pre>
<pre><code>## # A tibble: 0 x 0</code></pre>
<pre class="r"><code>df_chisq</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  .
## X-squared = 138.83, df = 16, p-value &lt; 2.2e-16</code></pre>
<p>Testing of association between subject_race and outcome. The p-value
is less than 0.05 so we reject the null hypothesis of no association and
conclude that there is a association between the row variables (race)
and column variables (outcome). Let’s have a look at the expected
counts:</p>
<pre class="r"><code>t1 &lt;- round(as_tibble(df_chisq$expected),0)
t1</code></pre>
<pre><code>## # A tibble: 5 x 5
##   arrest citation nothing summons warning
##    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1     19       33     402     630     163
## 2     25       43     526     824     213
## 3     99      168    2055    3223     832
## 4     20       34     412     646     167
## 5    147      247    3030    4753    1227</code></pre>
<p>Compare it to the actual counts in data:</p>
<pre class="r"><code>t2 &lt;- df %&gt;% 
  group_by(subject_race,outcome) %&gt;%  # the variables you want on the conteingency table
  summarise(n = n()) %&gt;% # need the totals
  mutate(proportion = n/sum(n)) %&gt;%
  select(-proportion) %&gt;% # Oh you definitely need to get rid of proportion here so it spreads properly
  spread(outcome, n) %&gt;%  # contingency table obtained! Also got proportions...and then got rid of them should make them separate
  ungroup() %&gt;% 
  select(-1)</code></pre>
<pre><code>## `summarise()` has grouped output by &#39;subject_race&#39;. You can override using the `.groups` argument.</code></pre>
<p>Let’s compare them side by side:</p>
<pre class="r"><code>t0 &lt;- df %&gt;% 
  group_by(subject_race,outcome) %&gt;%  # the variables you want on the conteingency table
  summarise(n = n()) %&gt;% # need the totals
  mutate(proportion = n/sum(n)) %&gt;%
  select(-proportion) %&gt;% # Oh you definitely need to get rid of proportion here so it spreads properly
  spread(outcome, n) %&gt;%  # contingency table obtained! Also got proportions...and then got rid of them should make them separate
  ungroup() %&gt;% 
  select(1)</code></pre>
<pre><code>## `summarise()` has grouped output by &#39;subject_race&#39;. You can override using the `.groups` argument.</code></pre>
<pre class="r"><code>t1a &lt;- tibble(t0,t1)
t2a &lt;- tibble(t0, t2)
kables(list(
            kable(caption = &quot;Expected counts&quot;, t1a) %&gt;% 
              kable_classic() %&gt;% 
                column_spec(3, color = spec_color(t1$arrest)),
            kable(caption = &quot;Actual counts&quot;, t2a) %&gt;% 
              kable_classic() %&gt;% 
                column_spec(3, color = spec_color(t2$arrest)))
       ) %&gt;% kable_classic()</code></pre>
<table class="kable_wrapper lightable-classic" style="font-family: &quot;Arial Narrow&quot;, &quot;Source Sans Pro&quot;, sans-serif; margin-left: auto; margin-right: auto;">
<tbody>
<tr>
<td>
<table class=" lightable-classic" style="font-family: &quot;Arial Narrow&quot;, &quot;Source Sans Pro&quot;, sans-serif; margin-left: auto; margin-right: auto;">
<caption>
Expected counts
</caption>
<thead>
<tr>
<th style="text-align:left;">
subject_race
</th>
<th style="text-align:right;">
arrest
</th>
<th style="text-align:right;">
citation
</th>
<th style="text-align:right;">
nothing
</th>
<th style="text-align:right;">
summons
</th>
<th style="text-align:right;">
warning
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
asian/pacific islander
</td>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;color: rgba(68, 1, 84, 1) !important;">
33
</td>
<td style="text-align:right;">
402
</td>
<td style="text-align:right;">
630
</td>
<td style="text-align:right;">
163
</td>
</tr>
<tr>
<td style="text-align:left;">
black
</td>
<td style="text-align:right;">
25
</td>
<td style="text-align:right;color: rgba(71, 19, 101, 1) !important;">
43
</td>
<td style="text-align:right;">
526
</td>
<td style="text-align:right;">
824
</td>
<td style="text-align:right;">
213
</td>
</tr>
<tr>
<td style="text-align:left;">
hispanic
</td>
<td style="text-align:right;">
99
</td>
<td style="text-align:right;color: rgba(39, 173, 129, 1) !important;">
168
</td>
<td style="text-align:right;">
2055
</td>
<td style="text-align:right;">
3223
</td>
<td style="text-align:right;">
832
</td>
</tr>
<tr>
<td style="text-align:left;">
other
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;color: rgba(69, 4, 87, 1) !important;">
34
</td>
<td style="text-align:right;">
412
</td>
<td style="text-align:right;">
646
</td>
<td style="text-align:right;">
167
</td>
</tr>
<tr>
<td style="text-align:left;">
white
</td>
<td style="text-align:right;">
147
</td>
<td style="text-align:right;color: rgba(253, 231, 37, 1) !important;">
247
</td>
<td style="text-align:right;">
3030
</td>
<td style="text-align:right;">
4753
</td>
<td style="text-align:right;">
1227
</td>
</tr>
</tbody>
</table>
</td>
<td>
<table class=" lightable-classic" style="font-family: &quot;Arial Narrow&quot;, &quot;Source Sans Pro&quot;, sans-serif; margin-left: auto; margin-right: auto;">
<caption>
Actual counts
</caption>
<thead>
<tr>
<th style="text-align:left;">
subject_race
</th>
<th style="text-align:right;">
arrest
</th>
<th style="text-align:right;">
citation
</th>
<th style="text-align:right;">
nothing
</th>
<th style="text-align:right;">
summons
</th>
<th style="text-align:right;">
warning
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
asian/pacific islander
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;color: rgba(71, 17, 100, 1) !important;">
15
</td>
<td style="text-align:right;">
394
</td>
<td style="text-align:right;">
676
</td>
<td style="text-align:right;">
150
</td>
</tr>
<tr>
<td style="text-align:left;">
black
</td>
<td style="text-align:right;">
34
</td>
<td style="text-align:right;color: rgba(64, 69, 136, 1) !important;">
25
</td>
<td style="text-align:right;">
481
</td>
<td style="text-align:right;">
868
</td>
<td style="text-align:right;">
223
</td>
</tr>
<tr>
<td style="text-align:left;">
hispanic
</td>
<td style="text-align:right;">
115
</td>
<td style="text-align:right;color: rgba(115, 208, 86, 1) !important;">
213
</td>
<td style="text-align:right;">
2084
</td>
<td style="text-align:right;">
3232
</td>
<td style="text-align:right;">
733
</td>
</tr>
<tr>
<td style="text-align:left;">
other
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;color: rgba(68, 1, 84, 1) !important;">
40
</td>
<td style="text-align:right;">
341
</td>
<td style="text-align:right;">
759
</td>
<td style="text-align:right;">
133
</td>
</tr>
<tr>
<td style="text-align:left;">
white
</td>
<td style="text-align:right;">
144
</td>
<td style="text-align:right;color: rgba(253, 231, 37, 1) !important;">
231
</td>
<td style="text-align:right;">
3125
</td>
<td style="text-align:right;">
4542
</td>
<td style="text-align:right;">
1362
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<pre class="r"><code># wow this is cool. Hope it knits well
# kable classic makes it nice, but kable_styling() makes it unreadable for my dark-mode/markdown setup
# Other options: kable_paper, kable_classic_2, kable_material, kable_material_dark
# this is not bad </code></pre>
<p>At a glance we can see that some of the biggest differences occur for
hispanic &amp; warning, and black &amp; citation. The ‘other’ group is
defined a bit differently between the dataset and wikipedia so we
shouldn’t draw too much from it.</p>
<pre class="r"><code># Cool kable concept that I want to explore later - but not now
#t1
#t1dt &lt;- lapply(t1[1:ncol(t1),2:nrow(t1)], function(x) {
#  cell_spec(x, bold = T,
#            color = spec_color(x, end = 0.9), #generates viridus color
#            font_size = spec_font_size(x, begin = 10, end = 16))
#})
#kbl(t1dt, escape = F, align = &quot;c&quot;) %&gt;% 
#  kable_classic(&quot;striped&quot;, full_width = F)</code></pre>
<p>And the most contributing cells to the total chi-square score:</p>
<pre class="r"><code>subject_race &lt;- c(&quot;asian/pacific islander&quot;, &quot;black&quot;, &quot;hispanic&quot;, &quot;other&quot;, &quot;white&quot;)
chisqres &lt;- as_tibble(df_chisq$residuals) %&gt;% add_column(subject_race, .before = 1)
chisqres</code></pre>
<pre><code>## # A tibble: 5 x 6
##   subject_race           arrest citation nothing summons warning
##   &lt;chr&gt;                   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1 asian/pacific islander -1.69     -3.10  -0.391   1.82   -0.994
## 2 black                   1.70     -2.73  -1.94    1.52    0.701
## 3 hispanic                1.56      3.51   0.640   0.158  -3.43 
## 4 other                  -3.12      1.10  -3.50    4.43   -2.62 
## 5 white                  -0.222    -1.03   1.72   -3.06    3.86</code></pre>
<p>The cells with the highest absolute standardized residuals contribute
the most to the total chi-square score. Let’s visualise this:</p>
<pre class="r"><code>ggplot(data = melt(chisqres), aes(x = subject_race,y = variable )) +
  geom_raster(aes(fill = value)) +
  scale_fill_gradient(low = &quot;green&quot;, high = &quot;red&quot;)</code></pre>
<pre><code>## Warning in melt(chisqres): The melt generic in data.table has been passed a tbl_df and will attempt
## to redirect to the relevant reshape2 method; please note that reshape2 is deprecated, and this
## redirection is now deprecated as well. To continue using melt methods from reshape2 while both
## libraries are attached, e.g. melt.list, you can prepend the namespace like reshape2::melt(chisqres).
## In the next version, this warning will become an error.</code></pre>
<pre><code>## Using subject_race as id variables</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-28-1.png" width="672" />
It can be seen that the column black is strongly associated with
summons/arrest/warning but not strongly associated with nothing and
citation.</p>
</div>
</div>
<div id="classification-models" class="section level3">
<h3>Classification models</h3>
</div>
</div>
<div id="multicollinearity" class="section level2">
<h2>Multicollinearity?</h2>
<pre class="r"><code>model_df &lt;- df %&gt;% 
  filter(raw_search_basis == &quot;Probable Cause (positive)&quot; | raw_search_basis == &quot;Probable Cause (negative)&quot;) %&gt;% 
  mutate(across(where(is_character), as_factor)) %&gt;% 
  select(subject_race, subject_sex, outcome, raw_search_basis, search_conducted) 
set.seed(42) 
rows &lt;- sample(nrow(model_df)) # 11970 rows
# We&#39;ll use the first 8000 randomised entries for the model
train &lt;- model_df[rows[1:8000], ]
test &lt;- model_df[-rows[1:8000], ]
# And the rest for testing
model &lt;- glm(raw_search_basis ~ subject_race, family = binomial(link = &quot;logit&quot;), data = train)
summary(model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = raw_search_basis ~ subject_race, family = binomial(link = &quot;logit&quot;), 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.0226  -0.8531  -0.8518   1.4033   1.6321  
## 
## Coefficients:
##                                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                        -0.51683    0.04219 -12.251  &lt; 2e-16 ***
## subject_racewhite                  -0.31039    0.05497  -5.646 1.64e-08 ***
## subject_raceother                  -0.30665    0.09978  -3.073  0.00212 ** 
## subject_raceblack                   0.14104    0.08774   1.607  0.10795    
## subject_raceasian/pacific islander -0.50861    0.10815  -4.703 2.57e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 10164  on 7999  degrees of freedom
## Residual deviance: 10102  on 7995  degrees of freedom
## AIC: 10112
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>We now have a model depicting the relationship between a search being
positive or negative and the race of the person being pulled over. The
p-values are interpreted as whether there is a difference between the
log-odds of the outcome between the intercept and the explanatory
variable. All of the predictors are significant using p &lt; 0.1 as the
criteria but if we use p &lt; 0.05 which is more standard then the model
would reject subject_raceblack as being useful in the model.</p>
<pre class="r"><code>calc_class_err = function(actual, predicted){
  mean(actual != predicted)
}
probabilities &lt;-  predict(model, newdata = test, type = &quot;response&quot;)
predicted.class &lt;- ifelse(probabilities &gt; 0.5, &quot;Probable Cause (negative)&quot;, &quot;Probable Cause (positive)&quot;)
test_table &lt;-  table(predicted.class, test$raw_search_basis)
test_table #&lt;- rbind(test_table, c(0,0))</code></pre>
<pre><code>##                            
## predicted.class             Probable Cause (positive) Probable Cause (negative)
##   Probable Cause (positive)                      2667                      1303</code></pre>
<pre class="r"><code># for prediction this model needs some adjustments
#rownames(test_table) &lt;-  c(&quot;Probable Cause (positive)&quot;,(&quot;Probable Cause (negative)&quot;))
test_table</code></pre>
<pre><code>##                            
## predicted.class             Probable Cause (positive) Probable Cause (negative)
##   Probable Cause (positive)                      2667                      1303</code></pre>
<pre class="r"><code>#alc_class_err(actual = test$raw_search_basis, predicted = predicted.class)
# this doesn&#39;t tell the whole story.</code></pre>
<p>There is cause for concern in the table as the predicted class is
only showing “Probable Cause (positive)” predictions. Lets obtain more
metrics.</p>
<pre class="r"><code>test1 &lt;- test %&gt;% mutate(raw_search_basis = ifelse(raw_search_basis == &quot;Probable Cause (negative)&quot;, 1, 0))
cutoffs &lt;- seq(0, 1, by = 0.05)
eff &lt;- sapply(cutoffs, function(cutoff){
  sum((probabilities &gt; cutoff) == test1$raw_search_basis)/length(probabilities)
})
eff</code></pre>
<pre><code>##  [1] 0.3282116 0.3282116 0.3282116 0.3282116 0.3282116 0.3282116 0.3576826 0.5853904 0.6556675
## [10] 0.6717884 0.6717884 0.6717884 0.6717884 0.6717884 0.6717884 0.6717884 0.6717884 0.6717884
## [19] 0.6717884 0.6717884 0.6717884</code></pre>
<pre class="r"><code># Produce a ROC curve based on the current model.
test_roc = roc(test$raw_search_basis ~ probabilities, plot = TRUE, print.auc = TRUE)</code></pre>
<pre><code>## Setting levels: control = Probable Cause (positive), case = Probable Cause (negative)</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<pre class="r"><code># 0.554 isn&#39;t very good. But we should use it to choose optimal threshold</code></pre>
<pre class="r"><code># extract the co-ordinates
mycoords &lt;- coords(test_roc, &quot;all&quot;)
best.coords &lt;- coords(test_roc, &quot;best&quot;, best.method = &quot;youden&quot;)
best.coords</code></pre>
<pre><code>##   threshold specificity sensitivity
## 1 0.3393102   0.6505437   0.4520338</code></pre>
<p>Sensitivity is defined as the True Positive rate (sensitivity = <span
class="math inline">\(\frac{TP}{TP + FN}\)</span>). This is decently
high for this model. Specificity is defined as the True Negate rate
(specificity = <span class="math inline">\(\frac{TN}{TN +
FP}\)</span>}). We use 1 - Specificity to define the False Positive
Rate. Lowering the classification threshold classifies more items as
‘positive’, increasing more false positives and true positives. Our
optimal cutoff here using the “youden” method is 0.339, this will
improve the classification power of the model.</p>
<pre class="r"><code>probabilities &lt;-  predict(model, newdata = test, type = &quot;response&quot;)
predicted.class &lt;- ifelse(probabilities &gt; 0.339, &quot;Probable Cause (negative)&quot;, &quot;Probable Cause (positive)&quot;)
test_table &lt;-  table(predicted.class, test$raw_search_basis)
test_table</code></pre>
<pre><code>##                            
## predicted.class             Probable Cause (positive) Probable Cause (negative)
##   Probable Cause (negative)                       932                       589
##   Probable Cause (positive)                      1735                       714</code></pre>
<p>Classification rate is <span
class="math inline">\((1735+589)/(1735+589+932+714) = 0.585\)</span>,
which is a lot better than 0.3 from earlier. The true positive rate is
<span class="math inline">\(1735/1735+932 = 0.65\)</span> which is the
same as the sensitivity rate stated. The true negative rate is <span
class="math inline">\(589/(589 + 714 = 0.45\)</span> which is the
specificity rate.</p>
</div>
<div id="model-fit" class="section level2">
<h2>Model fit</h2>
<p>(Move ROC stuff here).</p>
<p>There are two very different approaches to answer this question. One
is to get statistics on how well you can predict the dependent variable
based on the independent variables (measures of predictive power).
Examples would be R-square, the area under the ROC curve, and several
rank-order correlations. Higher is better but there is rarely a fixed
cut-off that distinguishes an acceptable model from one that is not
acceptable.</p>
<p>The other approach is to compute a goodness-of-fit statistic. These
test whether you can do better by making the model more complicated,
specifically, adding non-linearities, adding interactions or chainging
the link function. There are the deviance, Pearson chi-square or
Hosmer-Lemeshow test. These are tests of the null hypothesis that the
fitted model is correct, and their output is a p-value, with a higher
value indicating a better fit. A p-value below some specified <span
class="math inline">\(\alpha\)</span> level would suggest that the model
is not acceptable.</p>
<p>Important to note is that measures of predictive power and
goodness-of-fit statistics are testing different things. As such a model
with very high R-squared for example might have terrible goodness-of-fit
statistics.</p>
<p>A bit more on goodness-of-fit (GOF) tests: they test whether there
are any non-linearities or interactions. One can always produce a good
fit by adding enough interactions and nonlinearities. The question is if
you really need them to properly represent the data. GOF tests are
designed to answer that question.</p>
<p>Small summary of GOF tests:</p>
<ul>
<li>Deviance: different of likelihoods between fitted and saturated
model (saturated model likelihood is 1). Is always <span
class="math inline">\(\geq 0\)</span>, and is <span
class="math inline">\(0\)</span> if the fit is perfect.</li>
<li>&amp; Pearson chi-square</li>
<li>Hosmer and Lemeshow: groups cases together according to predicted
values from the logistic regression model. Predicted values are sorted
from highest to lowest, and then separated into several groups of
approximately equal size. The number of observed events and non-events
are calculated, as well as the expected number of events and non-events.
This would be the sum of the predicted probabilities and the expected
number of non-events would be the group size minus the expected number
of events. Pearson’s chi-square is applied to compare observed counts
with expected counts. Degrees of freedom is the number of groups minus
2. Has drawbacks - changing the number of groups can heavily influence
the outcome, and adding statistically significant interaction or
non-linearity to a model can sometimes increase the HL and reduce the
p-value. This suggest we’re better off without this statistically
significant interaction term. The reverse can also happen (adding a
non-significant interaction will improve the HL fit).</li>
</ul>
<p>Lets look at our measures of predictiveness:</p>
<pre class="r"><code>PseudoR2(model, which = c(&quot;McFadden&quot;, &quot;CoxSnell&quot;,&quot;Tjur&quot;))</code></pre>
<pre><code>##    McFadden    CoxSnell        Tjur 
## 0.006121411 0.007746915 0.007816934</code></pre>
<p>All of these suggest that the predictive power is poor - which is
definitely something we can agree on. The purpose of the model is to see
the interaction rather than the predictive power anyway.</p>
<p>Now let’s take a look at the goodness of fit measures.</p>
<pre class="r"><code># To see if the deviance matters we take the full model and compare to the current one:
model1 &lt;- glm(raw_search_basis ~., family = binomial,data = train)
summary(model1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = raw_search_basis ~ ., family = binomial, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0766  -0.9266  -0.7999   1.3818   1.7758  
## 
## Coefficients:
##                                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                        -0.70328    0.05938 -11.844  &lt; 2e-16 ***
## subject_racewhite                  -0.35205    0.05660  -6.220 4.98e-10 ***
## subject_raceother                  -0.31705    0.10225  -3.101 0.001931 ** 
## subject_raceblack                   0.15472    0.08923   1.734 0.082936 .  
## subject_raceasian/pacific islander -0.48982    0.10958  -4.470 7.83e-06 ***
## subject_sexmale                     0.07990    0.05296   1.509 0.131388    
## outcomewarning                      0.55468    0.06294   8.813  &lt; 2e-16 ***
## outcomecitation                     2.50182    0.21688  11.535  &lt; 2e-16 ***
## outcomenothing                      1.41199    0.36489   3.870 0.000109 ***
## outcomearrest                      -0.23198    0.24339  -0.953 0.340533    
## search_conducted                   -0.01848    0.17650  -0.105 0.916590    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 10164  on 7999  degrees of freedom
## Residual deviance:  9827  on 7989  degrees of freedom
## AIC: 9849
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>anova(model, model1)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: raw_search_basis ~ subject_race
## Model 2: raw_search_basis ~ subject_race + subject_sex + outcome + search_conducted
##   Resid. Df Resid. Dev Df Deviance
## 1      7995      10102            
## 2      7989       9827  6   274.56</code></pre>
<pre class="r"><code>model2 &lt;- glm(raw_search_basis ~ subject_race + outcome, family = &quot;binomial&quot;, data = train)
anova(model, model1, model2)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: raw_search_basis ~ subject_race
## Model 2: raw_search_basis ~ subject_race + subject_sex + outcome + search_conducted
## Model 3: raw_search_basis ~ subject_race + outcome
##   Resid. Df Resid. Dev Df Deviance
## 1      7995    10101.6            
## 2      7989     9827.0  6   274.56
## 3      7991     9829.3 -2    -2.29</code></pre>
<p>For a better model that fits the data, we would add “outcome” to the
variables. The deviance is also almost as good as the saturated model.
However this is not the purpose of the exercise so I will just leave it
as an aside.</p>
<p>Let’s plot our model:</p>
<pre class="r"><code>plot(allEffects(model)) # from effects package. typical = median is an alternative option</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-39-1.png" width="672" />
On the left hand side we have predicted probabilities for being a
probable cause (negative). It also includes 95% confidence interval
bars. There is a clear effect of black and hispanics being those more
likely to be false alarms.</p>
</div>
<div id="assumption-check" class="section level2">
<h2>Assumption check</h2>
<ul>
<li>outcome is binary/dichotomous</li>
<li>linear relationship between logic of the outcome and each predictor
variable</li>
<li>no influencial values</li>
<li>no multicollinearity</li>
</ul>
<pre class="r"><code>check_collinearity(model) # jokes only 1 variable</code></pre>
<pre><code>## Warning: Not enough model terms in the conditional part of the model to check for multicollinearity.</code></pre>
<pre><code>## NULL</code></pre>
<pre class="r"><code>out_list &lt;- check_outliers(train) # 218 outliers
dfbetas_model &lt;- as_tibble(dfbetas(model)[out_list == TRUE, ])
dfbetas_model %&gt;% 
  ggplot(aes(y = .[[1]], x = seq_along(dfbetas_model$`(Intercept)`))) +
  geom_point() </code></pre>
<pre><code>## Warning: Use of `dfbetas_model$`(Intercept)`` is discouraged. Use `(Intercept)` instead.</code></pre>
<pre><code>## Warning: Use of `.[[1]]` is discouraged. Use `.data[[1]]` instead.</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<pre class="r"><code>dfbetas_model %&gt;% 
  ggplot(aes(y = .[[2]], x = seq_along(subject_raceblack))) +
  geom_point() </code></pre>
<pre><code>## Warning: Use of `.[[2]]` is discouraged. Use `.data[[2]]` instead.</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-40-2.png" width="672" /></p>
<pre class="r"><code>dfbetas_model %&gt;% 
  ggplot(aes(y = .[[3]], x = seq_along(subject_raceblack))) +
  geom_point() </code></pre>
<pre><code>## Warning: Use of `.[[3]]` is discouraged. Use `.data[[3]]` instead.</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-40-3.png" width="672" />
The only thing we can comment on here is that we can’t remove these
outliers - they are an essential part of the data.</p>
<pre class="r"><code>plot(model$fitted.values)</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
</div>
<div id="interpretation-and-predictions" class="section level2">
<h2>Interpretation and Predictions</h2>
<p>Let’s quickly see how R is dummy coding the variables:</p>
<pre class="r"><code>contrasts(model_df$raw_search_basis)</code></pre>
<pre><code>##                           Probable Cause (negative)
## Probable Cause (positive)                         0
## Probable Cause (negative)                         1</code></pre>
<p>Probable cause (positive) is coded as 0 while probable cause
(negative) is coded as 1. We take exponential of the estimates for
easier interpretation. Some of the notable interptations are:</p>
<pre class="r"><code>model</code></pre>
<pre><code>## 
## Call:  glm(formula = raw_search_basis ~ subject_race, family = binomial(link = &quot;logit&quot;), 
##     data = train)
## 
## Coefficients:
##                        (Intercept)                   subject_racewhite  
##                            -0.5168                             -0.3104  
##                  subject_raceother                   subject_raceblack  
##                            -0.3066                              0.1410  
## subject_raceasian/pacific islander  
##                            -0.5086  
## 
## Degrees of Freedom: 7999 Total (i.e. Null);  7995 Residual
## Null Deviance:       10160 
## Residual Deviance: 10100     AIC: 10110</code></pre>
<ul>
<li>An intercept of -0.51 implies there is a <span
class="math inline">\(exp(-0.51) = 0.60\)</span> probability that the
pull-over for a hispanic is a false alarm</li>
<li>subject_raceblack estimate of 0.13792 means being black increases
the log odds by 0.13792. being black has <span
class="math inline">\(exp(0.13792) = 1.148\)</span> odds of being a
negative false alarm than hispanics (the reference or intercept
category)</li>
<li>All other groups have negative estimates and so they are more likely
to be Probable Cause(positive) than hispanics
<ul>
<li>White has an estimate of -0.33708, and so <span
class="math inline">\(exp(-0.33708) = 0.714\)</span> implies the average
white has a <span class="math inline">\(0.6*0.714= 0.42\)</span> chance
of being a Probable Cause (negative). Which means pullovers have more
than a 50% chance of being correct</li>
<li>Asian/Pacific Islander has an estimate of -0.49, and so <span
class="math inline">\(exp(-0.49) = 0.61\)</span> implies the average
asian/pacific islander has a <span class="math inline">\(0.6*0.61 =
0.366\)</span> chance of being a Probable Cause (negative). Which means
pullovers have a 36% chance of being a Probable Cause (negative). I.e
most of the pullovers due to suspicion were justified</li>
</ul></li>
</ul>
<pre class="r"><code># Showing this in quick view
exp(coef(model))</code></pre>
<pre><code>##                        (Intercept)                  subject_racewhite 
##                          0.5964096                          0.7331603 
##                  subject_raceother                  subject_raceblack 
##                          0.7359083                          1.1514688 
## subject_raceasian/pacific islander 
##                          0.6013296</code></pre>
<pre class="r"><code>ggplot(model_df, aes(raw_search_basis, fill = subject_race)) +
  geom_bar(position = &quot;fill&quot;)</code></pre>
<p><img src="Open_Police_CA_files/figure-html/unnamed-chunk-45-1.png" width="672" />
This makes sense when we look at the plot above, proportionally most of
the false alarms</p>
<p>Now lets see the classification rate when we test the predictive
ability of the model.</p>
<pre class="r"><code>probabilities &lt;-  predict(model, newdata = test, type = &quot;response&quot;)
predicted.class &lt;- ifelse(probabilities &gt; 0.327, &quot;Probable Cause (negative)&quot;, &quot;Probable Cause (positive)&quot;)
table(predicted.class, test$raw_search_basis)</code></pre>
<pre><code>##                            
## predicted.class             Probable Cause (positive) Probable Cause (negative)
##   Probable Cause (negative)                       932                       589
##   Probable Cause (positive)                      1735                       714</code></pre>
<p>There is a problem with this model. If we change 0.32 to be the
threshhold (based on ROC optimisation), then we get a 59% classification
success rate. We shouldn’t use this model for predicting anyway.</p>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>There is bias against black persons. In terms of model there isn’t
anything nice from logistic regression for prediction. Perhaps I should
try classification trees, neural networks or support vector machines
next.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
